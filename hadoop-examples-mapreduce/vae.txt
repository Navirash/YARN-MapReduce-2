Le concept d'autoencodeurs variationnels (VAE) est un cadre puissant d'apprentissage profond qui combine les principes des autoencodeurs et de la modélisation probabiliste. Les VAE ont été introduits par Kingma et Welling en 2013 et sont depuis largement utilisés pour la modélisation générative et les tâches d'apprentissage non supervisé.

À un niveau élevé, les VAE sont des modèles génératifs qui visent à apprendre la distribution sous-jacente d'un ensemble de données donné. Ils se composent d'un réseau d'encodage, d'un réseau de décodage et d'une représentation de l'espace latent. Les VAE apprennent à coder les données d'entrée dans un espace latent de dimension inférieure, où chaque point correspond à une représentation ou à un échantillon différent. Le décodeur apprend ensuite à générer de nouveaux échantillons en décodant les points de l'espace latent dans l'espace de données d'origine.

Le réseau de codage prend un échantillon d'entrée et l'associe aux paramètres d'une distribution de probabilité dans l'espace latent, généralement une distribution gaussienne multivariée. Cela permet au codeur de capturer les propriétés statistiques sous-jacentes des données d'entrée et de les coder dans une représentation compacte. L'espace latent agit comme un goulot d'étranglement, obligeant le codeur à capturer les caractéristiques les plus saillantes des données.

Pour générer de nouveaux échantillons, le réseau du décodeur prend un point de l'espace latent et l'inscrit dans l'espace de données d'origine. Le décodeur apprend à générer des échantillons qui ressemblent aux données d'entrée en minimisant la perte de reconstruction, qui mesure l'écart entre l'échantillon original et l'échantillon reconstruit. Cette perte encourage le décodeur à générer des échantillons réalistes et significatifs.

La principale innovation des VAE réside dans le processus de formation, qui consiste à maximiser l'objectif de la limite inférieure de l'évidence (ELBO). L'objectif ELBO se compose de deux termes : la perte de reconstruction et la divergence KL entre la distribution dans l'espace latent et une distribution préalable prédéfinie (généralement une distribution gaussienne standard). Le terme de divergence KL régularise l'espace latent, l'encourageant à suivre la distribution préalable et empêchant l'ajustement excessif.

Pendant l'apprentissage, les VAE optimisent simultanément les paramètres des réseaux du codeur et du décodeur afin de maximiser l'objectif ELBO. Le codeur apprend à capturer la distribution dans l'espace latent qui explique le mieux les données d'entrée, tandis que le décodeur apprend à générer des échantillons qui peuvent être reconstruits avec précision. Cette optimisation conjointe permet aux VAE d'apprendre des représentations significatives et démêlées des données.

Les VAE ont de nombreuses applications, telles que la génération d'images, la détection d'anomalies et la compression de données. Elles peuvent générer des échantillons nouveaux et divers en échantillonnant des points de l'espace latent et en les décodant. Les VAE permettent également d'interpoler entre différents échantillons de l'espace latent, ce qui permet des transitions douces et contrôlées dans les données générées. En outre, la représentation de l'espace latent apprise par les VAE peut être utilisée pour des tâches en aval telles que la classification ou le regroupement.

Traduit avec www.DeepL.com/Translator (version gratuite)
